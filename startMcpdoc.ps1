 uvx --from mcpdoc mcpdoc     --urls LangGraph:https://langchain-ai.github.io/langgraph/llms.txt     --transport sse     --port 8082     --host localhost



 # starts the llm router for cursor ollama runs instead of default
 # cursor (claude models)
 # ./llm-router-windows-amd64.exe -log-level debug -port 11434